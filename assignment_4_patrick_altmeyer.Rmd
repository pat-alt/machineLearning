---
title: "Problem Set 4"
author: "Patrick Altmeyer"
date: "`r format(Sys.Date(), '%d %B, %Y')`"
output: 
  bookdown::pdf_document2:
    toc: false
    number_sections: false
  bookdown::html_document2:
    code_folding: show
    number_sections: false
    toc: true
    toc_float: true
bibliography: bib.bib
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=50), message = FALSE)
library(data.table)
library(reticulate)
library(ggplot2)
source("R/utils.matrix2latex.R")
```

# Problem 13

<hr>

# Problem 14

<hr>

# Problem 15

The function below generates data with the desired distribution:

```{r, echo=TRUE}
sim_data <- function(n=100,d=5,a=0.5) {
  y <- 2*(rbinom(n,1,0.5)-0.5) # generate binary outcome
  X <- matrix(rnorm(n*d),n) + matrix(kronecker(y,matrix(c(a,rep(0,d-1)))),n,byrow = T) 
  return(list(X=X,y=y))
}
```

```{r}
n <- 100000
d <- 5
a <- 0.5
invisible(list2env(sim_data(n=n,d=d,a=a),envir = environment()))
dt <- data.table(y,X)
avg <- dt[,lapply(.SD,mean),by=y]
avg_1 <- matrix2latex(matrix(round(unlist(avg[y==1,2:(d+1)]),1)))
avg_2 <- matrix2latex(matrix(round(unlist(avg[y==-1,2:(d+1)]),1)))
setnames(dt, sprintf("V%i",1:d), sprintf("X%i",1:d))
dt <- melt(dt, id.vars = "y")
```

Figure \@ref(fig:class) provides a quick sanity check: it plots the class-conditional densities of $\mathbf{X}$ where $d=`r d`$, $n=`r n`$ and $a=`r a`$. The data looks normally distributed and the vectors of class conditional empirical means rounded to the nearest decimal are

$$
\begin{aligned}
&& \mathbf{\bar{X}}_{\mathbf{y}=1} = `r avg_1` &, \mathbf{\bar{X}}_{\mathbf{y}=-1} = `r avg_2` \\
\end{aligned}
$$

```{r class, fig.width=10, fig.height=2.3, fig.cap="Class-conditional empirical densities."}
p <- ggplot(data=dt, aes(x=value, fill=factor(y))) +
  geom_density(alpha=0.25) +
  facet_grid(cols=vars(variable)) +
  scale_fill_discrete("Outcome:") +
  labs(
    x="X",
    y="Conditional density"
  )
p
```

Linear classification through stochastic gradient descent can be implemented as below. 

```{r, code=readLines("R/stochastic_gradient_descent.R"), eval=FALSE, echo=TRUE}
```

```{r}
d <- 1
n <- 100
a <- 0
invisible(list2env(sim_data(n=n,d=d,a=a),envir = environment()))
```

<!-- To illustrate how it works, let us begin with a simple 2-$d$ example where $\mathbf{y}$ is perfectly linearly separable given $\mathbf{x}$. In this case, the problem becomes trivial for binary $y$, so instead to make things a little more interesting at this point, let us add some additional noise for a moment. In particular, let $y_i \in \{-1+\varepsilon_i,1+\varepsilon_i \}$. In this context linear separability can still be achieved by choosing a high value for $a$, say $a = `r a`$. -->

<!-- ```{r} -->
<!-- plot(X,y) -->
<!-- abline(a=0,b=0,col="red") -->
<!-- ``` -->

```{r gridsearch}
source("R/linear_classifier.R")
set.seed(111)
d <- round(exp(seq(1,5,length.out=10)))
n <- round(exp(seq(3,11,length.out=25)))
n_iter <- c(10,100,1000)
a <- c(0.001,0.1,0.5,1,5)
eta <- c(1e-2,1e-5,1e-10,1e-20)
surrogate <- c("bayes","exp", "hinge", "log")
J <- 1 # number of independent test sets
grid <- data.table(expand.grid(n=n,d=d,a=a,eta=eta,n_iter=n_iter,loss=surrogate))
grid <- grid[sample(1:nrow(grid),20),]
output <- rbindlist(
  lapply(
    1:nrow(grid),
    function(i) {
      invisible(list2env(c(grid[i,]), envir = environment()))
      train <- sim_data(n,d,a) # training data 
      X_train <- train[["X"]]
      y_train <- train[["y"]]
      test <- sim_data(n,d,a) # test data (same size)
      X_test <- test[["X"]]
      y_test <- test[["y"]]
      performance <- rbindlist(
        lapply( # repeat J times since optimization is stochastic
          1:J,
          function(j) {
            # SGD:
            if (loss=="bayes") {
              fitted_train <- sign(X_train %*% c(1,rep(0,d-1)))
              fitted_test <- sign(X_test %*% c(1,rep(0,d-1)))
            } else {
              mod <- linear_classifier(X=X_train, y=y_train, eta=eta, loss=loss, n_iter = n_iter)
              fitted_train <- predict(mod)
              fitted_test <- predict(mod, newdata = X_test)
            }
            error_rate_train <- sum(fitted_train * y_train < 0)/n
            error_rate_test <- sum(fitted_test * y_test < 0)/n
            performance <- data.table(
              n = n,
              j = j,
              d = d,
              n_iter = n_iter,
              eta,
              loss = loss,
              error_rate_train = error_rate_train,
              error_rate_test = error_rate_test
            )
            return(performance)
          }
        )
      )
      return(performance)
    }
  )
)
saveRDS(output, file="data/sgd.rds")
```


<hr>