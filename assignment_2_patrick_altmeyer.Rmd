---
title: "Problem Set 2"
author: "Patrick Altmeyer"
date: "`r format(Sys.Date(), '%d %B, %Y')`"
output: 
  bookdown::pdf_document2:
    toc: false
  bookdown::html_document2:
    code_folding: show
    number_sections: false
    toc: true
    toc_float: true
bibliography: bib.bib
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(data.table)
```

# Piece-wise CDFs

We are given the conditional CDFs 

![](www/ps2_5_cdf.png)

from which the conditional PDFs can be derived:

![](www/ps2_5_pdf.png)

Then we have for the joint PDF:

![](www/ps2_5_joint_pdf.png)

Using Bayes rule we can then determine a functional form for $\eta(\mathbf{X})$

![](www/ps2_5_eta.png)

which can be illustrated as follows:

![](www/ps2_5_eta_chart.png)

## Bayes classifier and risk

Then we have for the Bayes classifier and corresponding risk:

![](www/ps2_5_bayes.png)

## 1-NN

For the 1-NN we can not that the following holds asymptotically:

![](www/ps2_5_nn_logic.png)

And consequently applying this here we get:

![](www/ps2_5_1nn.png)

## 3-NN

Similarly, we can show for the 3-NN rule:

![](www/ps2_5_3nn.png)

# Nearest neighbor regression

We can derive the optimal predictor as follows:

![](www/ps2_6_optimal_predictor.png)

In other words, the optimal predictor $f^*$ of $\mathbf{Y}$ given $\mathbf{X}$ is just the conditional mean of $\mathbf{Y}$ given $\mathbf{X}$.

```{r}
source("R/knn_regressor.R")
```

```{r, eval=FALSE}
set.seed(111)
d <- round(exp(1:4))
n <- round(exp(4:8))
k <- c(1,3,5,7,9)
J <- 100 # number of independent samples
sigma <- 0.1
grid <- data.table(expand.grid(n=n,d=d,k=k))
output <- rbindlist(
  lapply(
    1:nrow(grid),
    function(i) {
      list2env(c(grid[i,]), envir = environment())
      beta <- abs(c(1,rep(rnorm(d,mean = 0.2))))
      performance <- rbindlist(
        lapply( # loop over J samples 
          1:J,
          function(j) {
            X <- matrix(rnorm(d*n),n)
            y <- cbind(1,X) %*% beta + rnorm(n,sd = sigma)
            fitted <- knn_regressor(X,y,k)
            mse <- mean((y-fitted)^2)
            performance <- data.table(
              k = k,
              n = n,
              j = j,
              d = d,
              mse = mse
            )
            return(performance)
          }
        )
      )
      return(performance)
    }
  )
)
saveRDS(output, file="data/knn_reg_errors.rds")
```

```{r}
library(ggplot2)
output <- readRDS(file="data/knn_reg_errors.rds")
ggplot(data=output[,.(mse=mean(mse)),by=.(k,n,d)], aes(x = log(n), y=mse)) +
  geom_line() +
  geom_point() +
  facet_grid(
    rows = vars(d),
    cols = vars(k),
    scales = "free_y"
  )
```


# Bayes risk

# NN for binary classification

## Maths

Bayes risk and the asymptotic risks of the different KNN classifiers involves an expectation with respect to $\eta(\mathbf{X})= \frac{x^{(1)}+x^{(2)}}{2}$. To solve for that we first need to know the density of $z=x^{(1)}+x^{(2)}$. It turns out that $z$ follows a *triangular* distribution with density

![](www/ps2_8_density.png)


This can be shown as follows:

![](www/ps2_8_derivation_density.png)

Then consider the following two cases: (1) $0<z\le1$ and (2) $1<z<2$. For these cases we have the following:

![](www/ps2_8_2cases.png)
Using this density function we can now solve for the expectation.

### Bayes risk

The Bayes risk is $R^*= \frac{1}{3}$ which can be computed as follows:

![](www/ps2_8_bayes.png)

### 1NN

The risk of the 1NN classifier is $R^{1NN}= \frac{5}{12}$ which can be shown as follows:

![](www/ps2_8_1nn.png)

A quick sanity check shows that 

$$
\begin{aligned}
&& R^{1NN}&=\frac{5}{12}< \frac{4}{9} = 2 R^*(1-R^*)
 \\
\end{aligned}
$$

### 3NN

The risk of the 3NN classifier is $R^{3NN}= \frac{47}{120}$ which can be shown as follows:

![](www/ps2_8_3nn.png)

In conclusion we have that:

$$
\begin{aligned}
&& R^*&= \frac{40}{120} < R^{3NN}= \frac{47}{120} < R^{1NN}= \frac{50}{120}\\
\end{aligned}
$$


## Program

```{r}
source("R/knn_classifier.R")
```


```{r}
sim_data <- function(n,d) {
  X <- matrix(runif(n*d),n) # uniform 0,1
  p_y <- rowSums(X[,1:2])/2 # probabilities of each Bernoulli trial
  y <- rbinom(n, 1, p_y)
  return(list(X=X,y=y))
}
```


```{r, eval=F}
set.seed(111)
d <- round(exp(1:4))
n <- round(exp(4:8))
k <- c(1,3,5,7,9)
J <- 100 # number of independent samples
grid <- data.table(expand.grid(n=n,d=d,k=k))
output <- rbindlist(
  lapply(
    1:nrow(grid),
    function(i) {
      list2env(c(grid[i,]), envir = environment())
      performance <- rbindlist(
        lapply( # loop over J samples 
          1:J,
          function(j) {
            list2env(sim_data(n,d),envir = environment())
            fitted <- knn_classifier(X,y,k)
            prob_error <- sum(y!=fitted)/n
            performance <- data.table(
              k = k,
              n = n,
              j = j,
              d = d,
              prob_error = prob_error
            )
            return(performance)
          }
        )
      )
      return(performance)
    }
  )
)
saveRDS(output, file="data/knn_errors.rds")
```

```{r}
library(ggplot2)
output <- readRDS(file="data/knn_errors.rds")
ggplot(data=output[,.(prob_error=mean(prob_error)),by=.(k,n,d)], aes(x = log(n), y=prob_error)) +
  geom_line() +
  geom_point() +
  facet_grid(
    rows = vars(d),
    cols = vars(k)
  )
```

